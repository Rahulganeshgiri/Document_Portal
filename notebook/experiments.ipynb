{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9b2ac01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e5cc0763",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm=ChatGroq(model=\"llama-3.1-8b-instant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4137902d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "\n",
    "loader = PyPDFLoader(r\"C:\\Users\\Rahul\\OneDrive\\Desktop\\Document_Portal\\notebook\\interview-questions.pdf\")\n",
    "doc = loader.load()\n",
    "len(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09afdc9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='Embeddings,  Vector  DB,  Retrieval,  and  Similarity:  ●  What  embedding  and  vector  DB  did  you  use  and  why?  ●  What  was  the  vector  size  and  what  is  the  impact  of  vector  length?  ●  Which  vector  DB  did  you  use  and  why?  ●  What  are  different  types  of  similarity  search  (cosine,  Euclidean,  Manhattan)  and  when  to  \\nuse\\n \\nwhat?\\n ●  How  to  perform  retrieval  operation?  ●   How  do  you  handle  metadata  in  vector  DB?  ●  What  is  a  vector  DB?  \\nRAG  (Retrieval-Augmented  Generation):  ●   What  is  RAG  architecture?  ●  How  does  RAG  work?  ●   What  are  RAG  failures,  and  how  do  you  evaluate  RAG?  ●   Where  does  the  evaluation  module  sit  in  a  RAG  pipeline?  ●  How  to  design  Multi-modal  RAG?  ●  What  is  RAG  and  Agents?  ●  What  applications  have  you  built  using  RAG,  LangChain,  LangGraph?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='Deterministic  &  Guarded  Responses:  ●  How  to  ensure  deterministic  response  in  tightly  coupled  guideline-based  apps?  ●  How  to  define  guardrails  in  LLM  responses?  \\nConversational  AI:  ●  How  is  LLM  chatbot  different  from  normal  chatbot?  ●  How  is  LLM  chatbot  different  from  voice  bots?  ●   How  to  build  full-fledged  conversational  AI  system?  ●  What  is  LangGraph?  ●  What  is  agentic  flow  and  how  to  design  it?    \\nTech  Stack  &  Infra  Integration  Azure  &  Outlook  Flow:  ●  How  system  fetches  PDF  from  Outlook?  ●   Why  use  Azure  Blob  Storage?  ●   What  is  Microsoft  Graph  API?  ●   Role  of  Azure  Functions  or  App  Services?  ●   Why  use  Azure  Cosmos  DB?  ●   What  is  Azure  AI  Search  /  Azure  AI  Studio?  These  are  spot-on  for  cloud-based  GenAI  apps.  Keep  Azure  infra  knowledge  strong.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 0, 'page_label': '1'}, page_content='OCR  &  Parsing  ●   How  OCR  works  (including  LLM-based)?  ●   What  happens  after  data  extraction?  ●   How  to  parse  a  table  split  across  multiple  pages?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content='●  What  is  document  parsing  —  how  to  parse  from  documents  and  DBs?  Smart  Tip:  For  multi-page  table  parsing:  discuss  layout-aware  parsing  (like  PDFPlumber,  \\nunstructured.io,\\n \\nlayoutLM)\\n \\n—\\n \\nnot\\n \\njust\\n \\nLLMs\\n.\\n   \\nLLM  Understanding  &  Comparison  ●   What  is  BERT  vs  LLM?  (repeated  but  valid)  ●   How  LLM  is  different  from  BERT?  ●   Token  size  used  in  LLM  input?  ●  Which  LLMs  have  you  used?  ●  Gemini  vs  GPT-4.0?  ●  Deploying  Gemini  4.0-based  RAG  on  Azure/GCP?    \\nModel  Performance,  Accuracy  &  Retraining  ●  ML  metrics  for  classification?  ●   How  to  check  model  accuracy?  ●   What  do  you  do  if  accuracy  reduces?  ●  How  to  retrain  &  split  train-test  data?  Tip:  Be  ready  with  precision,  recall,  F1,  ROC-AUC,  and  confusion  matrix  based  use-cases.'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 1, 'page_label': '2'}, page_content=\"AI  System  Design  /  XAI  /  Production  ●   How  to  manage  concurrency  for  multiple  users?  ●  How  will  you  implement  memory  management?  ●   How  to  manage  cache  /  state?  ●   How  to  implement  XAI  /  Responsible  AI?  ●  How  to  define  &  enforce  guardrails?  ●   All  AI  use  cases  you've  worked  on?   \\nDataset  &  Chunking  \\n●  How  did  you  profile  your  dataset  before  processing  —  number  of  rows,  columns,  data  \\ntypes,\\n \\nmissing\\n \\nvalues?\\n \\n ●  Why  did  you  chunk  a  ~500k  row  dataset  even  though  LLMs  can  handle  small  datasets?  \\n ●  What  chunking  strategies  (fixed,  recursive,  semantic)  did  you  consider,  and  when  is  each  \\nideal?\\n \\n ●  What  impact  does  vector  size/dimension  have  on  retrieval  quality  and  performance?\"),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='Embeddings,  Vector  DB  &  Retrieval  \\n●  Which  embedding  model  (OpenAI,  BGE,  etc.)  and  vector  DB  (FAISS,  Pinecone,  etc.)  did  \\nyou\\n \\nuse\\n \\nand\\n \\nwhy?\\n \\n ●  What  types  of  vector  stores  exist,  and  when  should  you  use  FAISS,  Pinecone,  Weaviate,  \\nor\\n \\nQdrant?\\n \\n ●  What  indexing  methods  (Flat,  IVF,  PQ,  HNSW)  does  FAISS  support,  and  how  do  they  \\naffect\\n \\nspeed/accuracy?\\n \\n ●  How  are  vectors  stored  internally  in  vector  databases?  \\n ●  How  is  a  vector  retrieved  (via  similarity  search),  and  what  happens  under  the  hood?  \\n ●  How  does  product  quantization  and  inverted  indexing  make  large-scale  search  more  \\nefficient?\\n \\n ●  How  did  you  optimize  search  performance  with  ~800k  rows?  \\n ●  What  similarity  metrics  (cosine,  dot  product,  Euclidean,  Manhattan)  did  you  explore,  and  \\nwhen\\n \\nis\\n \\neach\\n \\nideal?\\n \\n ●  When  would  you  choose  a  managed  vector  DB  like  Pinecone  over  a  local  one  like'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 2, 'page_label': '3'}, page_content='when\\n \\nis\\n \\neach\\n \\nideal?\\n \\n ●  When  would  you  choose  a  managed  vector  DB  like  Pinecone  over  a  local  one  like  \\nFAISS?\\n \\nRAG  (Retrieval-Augmented  Generation)  \\n●  What  is  RAG  architecture  and  how  did  you  implement  it  in  your  system?  \\n ●  How  do  you  evaluate  and  improve  a  RAG  pipeline  when  responses  are  inaccurate  or  \\nhallucinated?\\n \\n ●  Where  does  the  RAG  evaluation  module  sit,  and  what  metrics  do  you  use  to  validate  \\nresponses?\\n \\n ●  What  different  similarity  search  strategies  are  used  in  RAG,  and  which  is  best  when?  \\n ●  What  is  reranking  (e.g.,  MMR,  cross-encoder),  and  when  is  it  needed  in  RAG?  \\n ●  What  is  agentic  RAG  and  how  does  it  differ  from  classic  retrieval  pipelines?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='●  What  models/tools  (LangGraph,  LangChain,  FAISS,  OpenAI,  Azure)  did  you  use  to  build  \\nthe\\n \\nRAG\\n \\nsystem?\\n \\n ●  What  is  LangGraph,  and  how  is  it  different  from  LangChain  in  terms  of  agent  \\norchestration?\\n \\nPrompting,  JSON  Output,  LLM  Behavior  \\n●  What  is  the  token  limit  of  GPT-4,  and  how  does  it  affect  chunking  and  prompt  design?  \\n ●  What’s  the  difference  between  zero-shot  and  few-shot  prompting,  and  when  is  each  \\nideal?\\n \\n ●  What  are  the  drawbacks  of  few-shot  prompting  (e.g.,  cost,  prompt  drift,  token  \\nexplosion)?\\n \\n ●  How  do  you  reduce  hallucinations  in  LLMs  when  handling  scientific  or  sensitive  content?  \\n ●  How  do  you  ensure  the  LLM  returns  output  in  valid  JSON  or  structured  format  every  \\ntime?\\n \\n ●  How  do  you  improve  chain-of-thought  and  reasoning  quality  if  LLM  outputs  poor  \\nresponses?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 3, 'page_label': '4'}, page_content='time?\\n \\n ●  How  do  you  improve  chain-of-thought  and  reasoning  quality  if  LLM  outputs  poor  \\nresponses?\\n \\n ●  How  many  tokens  were  you  passing  to  the  LLM  on  average,  and  how  did  you  manage  \\ninput\\n \\nlimits?\\n \\n \\nConversational  AI  &  Agent  Design  \\n●  How  is  an  LLM  chatbot  different  from  a  rule-based  or  traditional  chatbot?  \\n ●  How  would  you  implement  role-based  access  (e.g.,  restrict  responses  based  on  \\nemployee\\n \\npay\\n \\ngrade)?\\n \\n ●  Have  you  worked  on  voice  bots,  and  how  do  they  differ  in  architecture  from  chatbots?  \\n ●  How  would  you  design  a  full-fledged  end-to-end  conversational  AI  system  using  \\nLangGraph\\n \\nor\\n \\nLangChain?\\n \\n ●  What  is  an  agentic  flow  and  how  do  you  design  multi-agent  workflows  using  LangGraph?  \\n ●  How  would  you  implement  session  memory  or  chat  history  in  a  multi-turn  chatbot?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='●  How  do  you  manage  state  and  cache  in  a  high-concurrency  GenAI  application?  \\n ●  How  do  you  scale  your  system  for  many  simultaneous  users  (concurrency  strategy)?  \\nApp  Integration  &  Infra  (Azure,  Email,  Parsing)  \\n●  How  did  your  system  automatically  detect  and  extract  PDF  files  from  Outlook?  \\n ●  Why  did  you  use  Azure  Blob  Storage  —  what  benefit  did  it  bring  to  your  pipeline?  \\n ●  What  does  Microsoft  Graph  API  do  in  your  architecture?  \\n ●  What’s  the  role  of  Azure  Functions  or  App  Services  in  your  RAG-based  solution?  \\n ●  What  is  Azure  AI  Search  and  how  does  it  work  with  vector-based  search?  \\n ●  Why  did  you  use  Azure  Cosmos  DB  instead  of  MongoDB  or  SQL?  \\n ●  How  did  you  parse  multi-page  tables  in  DOCX/PDF  files  (cost-efficient  +  accurate)?  \\n ●  What  steps  did  your  system  follow  after  extracting  data  via  OCR  (structured  parsing)?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 4, 'page_label': '5'}, page_content='●  What  steps  did  your  system  follow  after  extracting  data  via  OCR  (structured  parsing)?  \\n \\nML  Model  Metrics,  Accuracy  &  Retraining  \\n●  What  classification  metrics  (accuracy,  precision,  recall,  F1)  did  you  use  and  why?  \\n ●  If  model  accuracy  dropped,  how  did  you  debug  and  improve  the  pipeline?  \\n ●  How  do  you  retrain  an  ML  model,  and  how  do  you  manage  train/test  split  to  avoid  \\nleakage?\\n \\n ●  How  do  you  check  and  measure  model  accuracy,  both  for  LLMs  and  ML  models?  \\n \\nData  Structures  &  Algorithms  (DSA)  \\n●  What  are  the  best  and  worst-case  time  complexities  for  common  list  operations?  \\n ●  What’s  the  time  complexity  for  Python  list  operations  like  append,  insert,  pop,  etc.?  \\n ●  Which  is  faster  —  list  or  dictionary  —  and  in  what  scenarios?'),\n",
       " Document(metadata={'producer': 'Skia/PDF m143 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'interview-questions', 'source': 'C:\\\\Users\\\\Rahul\\\\OneDrive\\\\Desktop\\\\Document_Portal\\\\notebook\\\\interview-questions.pdf', 'total_pages': 6, 'page': 5, 'page_label': '6'}, page_content='GenAI  Project  Discussion  \\n●  Walk  me  through  your  latest  Generative  AI  project  (business  problem,  technical  flow,  \\noutcomes).\\n \\n ●  What  LLM  models,  vector  DBs,  tools,  and  cloud  services  did  you  use?  \\n ●  How  did  you  implement  Human-in-the-Loop  in  your  system  to  improve  quality  and  trust?  \\n ●  How  did  you  integrate  Responsible  AI  principles  (e.g.,  explainability,  fairness,  scientific  \\nvalidity)?\\n \\n ●  How  did  you  extract  structured  data  from  unstructured  documents  (e.g.,  research  \\nPDFs)?\\n \\n ●  What  was  the  structure  of  the  tech  team,  and  what  was  your  exact  role?  \\n ●  How  did  your  pipeline  handle  scale,  latency,  and  large  document  parsing?  \\nBehavioral  +  Guesstimate  \\n●  Guesstimate:  What  is  Netflix’s  annual  revenue?  (Show  step-by-step  thinking:  users  ×  \\nARPU)\\n \\n ●  If  we  call  your  manager  right  now,  what  are  3  strengths  and  3  improvement  areas  they’d  \\nshare?')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = splitter.split_documents(doc)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06013b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "LANGCHAIN_API_KEY=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "Gemini_API_KEY=os.getenv(\"Gemini_API_KEY\")\n",
    "OPENAI_API_KEY=os.getenv(\"OPENAI_API_KEY\")\n",
    "HuggingFace_API_KEY=os.getenv(\"HuggingFace_API_KEY\")\n",
    "GROQ_API_KEY=os.getenv(\"GROQ_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "08850b66",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rahul\\OneDrive\\Desktop\\Document_Portal\\env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Rahul\\OneDrive\\Desktop\\Document_Portal\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Rahul\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "import os\n",
    "# ensure token is available to huggingface-hub as HUGGINGFACEHUB_API_TOKEN\n",
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = HuggingFace_API_KEY\n",
    "\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ccef1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x251d7c60050>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "vectorstore = FAISS.from_documents(chunks, embedding_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d48f3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt_template = \"\"\"\n",
    "        Answer the question based on the context provided below. \n",
    "        If the context does not contain sufficient information, respond with: \n",
    "        \"I do not have enough information about this.\"\n",
    "\n",
    "        Context: {context}\n",
    "\n",
    "        Question: {question}\n",
    "\n",
    "        Answer:\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f201c4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9b4ca2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1f1a8b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "67144208",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(doc):\n",
    "    return \"\\n\\n\".join([lines.page_content for lines in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c01bf101",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | PROMPT\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "841f44dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This document appears to be a discussion about a Generative AI project, including its technical details, implementation, and challenges. It covers various aspects of the project, such as the use of Large Language Models (LLMs), vector databases, and cloud services, as well as the integration of Responsible AI principles, Human-in-the-Loop, and other techniques to improve quality and trust.\\n\\nThe document also touches on topics like LLM understanding and comparison, model performance, accuracy, and retraining, as well as conversational AI, tech stack, and infrastructure integration. It seems to be a comprehensive overview of the project's architecture, design, and implementation.\\n\\nOverall, this document is likely a documentation of a research project or a technical discussion about a Generative AI project, possibly for a academic or industry setting.\""
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag_chain.invoke(\"What is this document is really about?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e5e71f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
